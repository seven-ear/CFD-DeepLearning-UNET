{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imutils\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from pysim import config\n",
    "import glob\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original data:\", config.ORIG_INPUT_DATASET)\n",
    "print(\"INFO: This data is saved as tf.records so no further processing is done on disk.\")\n",
    "print(\"Folders\", config.TRAIN_PATH,config.VAL_PATH,config.TEST_PATH, \"will not be created.\")\n",
    "print(\"INFO: Further data processing is done on the fly.\")\n",
    "filename = os.path.sep.join([config.ORIG_INPUT_DATASET, config.filename])\n",
    "print(\"Following file is processed:\", config.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print examples to see how data was encoded\n",
    "raw_records = tf.data.TFRecordDataset(filename)\n",
    "for raw_record in raw_records.take(2):\n",
    "  example = tf.train.Example()\n",
    "  example.ParseFromString(raw_record.numpy())\n",
    "  # print(example) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tfrecord_parse(example):\n",
    "    tfrecord_format = {\"boundary\": tf.io.FixedLenFeature([], tf.string),\n",
    "                        \"sflow\": tf.io.FixedLenFeature([], tf.string)}\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format) \n",
    "    \n",
    "    boundary = tf.io.decode_raw(example[\"boundary\"], tf.uint8)  \n",
    "    boundary = tf.cast(boundary, tf.float32)\n",
    "    boundary = tf.reshape(boundary, [128, 256, 1])\n",
    "    #boundary = tf.image.grayscale_to_rgb(boundary) \n",
    "\n",
    "    sflow = tf.io.decode_raw(example[\"sflow\"], tf.float32)\n",
    "    sflow = tf.cast(sflow, tf.float32)\n",
    "    sflow = tf.reshape(sflow, [128, 256, 2])\n",
    "    #sflow = sflow[:, :, 0]\n",
    "    #sflow = tf.reshape(sflow, [128, 256, 2])\n",
    "    return boundary, sflow\n",
    "def load_dataset(filename):\n",
    "  dataset = tf.data.TFRecordDataset(filename)\n",
    "  dataset = dataset.map(partial(_tfrecord_parse))\n",
    "  return dataset\n",
    "\n",
    "def get_dataset(filename, batch_size):\n",
    "  dataset = load_dataset(filename)\n",
    "  dataset = dataset.shuffle(1024)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  return dataset\n",
    "\n",
    "dataset = get_dataset(filename, config.BATCH_SIZE)\n",
    "print(\"INFO: TFRecords file was parsed, and prepared for shuffling/batching!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for record in dataset:\n",
    "    i = i + 1\n",
    "print (\"INFO: There are\", i * config.BATCH_SIZE, \"samples in dataset!!!\")    \n",
    "\n",
    "train_size = int(config.TRAIN_SPLIT * i)\n",
    "val_size = int(config.VAL_SPLIT * i)\n",
    "test_size  = i - train_size - val_size\n",
    "\n",
    "# Split data in train, val, test on the fly!!!\n",
    "train_dataset = dataset.take(train_size).repeat(100)\n",
    "remaining     = dataset.skip(train_size)\n",
    "val_dataset = dataset.take(val_size).repeat(100)\n",
    "remanining    = dataset.skip(val_size)\n",
    "test_dataset  = dataset.take(test_size)\n",
    "\n",
    "#boundary_batch, sflow_batch = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook build_dataset.ipynb to script\n",
      "[NbConvertApp] Writing 3943 bytes to build_dataset.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script build_dataset.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
